import { serve } from "https://deno.land/std@0.140.0/http/server.ts";
import { streamText, generateText } from "npm:ai";
import { crypto } from "https://deno.land/std/crypto/mod.ts";
import { config } from "https://deno.land/x/dotenv/mod.ts";
import { findModel, MODELS } from "./models.ts";
import * as cache from "./cache.ts";
import { colors, timestamp, timeTrack } from "./cli.ts";

await config({ export: true });


const handler = async (request: Request): Promise<Response> => {
  const startTime = Date.now();
  const requestId =
    colors.cyan + `[${crypto.randomUUID().slice(0, 8)}]` + colors.reset;

  if (request.method === "GET") {
    return new Response("Hello World");
  }

  if (request.method === "POST") {
    try {
      const payload = (await request.json()) as {
        messages: Array<{ role: string; content: string }>;
        system?: string;
        model: string;
        max_tokens: number;
        stop?: string;
        stream: boolean;
        max_completion_tokens?: number;
        abortSignal?: AbortSignal;
      };

      // Log request details with colors
      console.log(
        `${timestamp()} ${requestId} ${colors.blue}üìù New request:${colors.reset} ${colors.bright}${payload.model}${colors.reset} | ${timeTrack(startTime)}`,
      );
      console.log(
        `${timestamp()} ${requestId} ${colors.magenta}üí≠ System:${colors.reset} ${payload.system?.slice(0, 100)}...`,
      );
      console.log(
        `${timestamp()} ${requestId} ${colors.yellow}üí¨ Last message:${colors.reset} ${payload.messages[payload.messages.length - 1].content.slice(0, 100)}...`,
      );

      const cacheKey = await cache.hashKey(JSON.stringify(payload));
      const cachedResult = await cache.loadItem(cacheKey);
      if (cachedResult) {
        console.log(
          `${timestamp()} ${requestId} ${colors.green}‚ö°Ô∏è Cache hit!${colors.reset} | ${timeTrack(startTime)}`,
        );
        const lastMessage =
          cachedResult.messages[cachedResult.messages.length - 1];
        return new Response(JSON.stringify(lastMessage), {
          headers: { "Content-Type": "application/json" },
        });
      }

      const modelConfig = findModel(payload.model);
      if (!modelConfig) {
        console.warn(
          `${timestamp()} ${requestId} ${colors.yellow}‚ö†Ô∏è  Unsupported model:${colors.reset} ${payload.model}`,
        );
        return new Response(JSON.stringify({ error: `Unsupported model: ${payload.model}`, availableModels: Object.keys(MODELS) }), {
          status: 400,
          headers: { "Content-Type": "application/json" },
        });
      }

      console.log(
        `${timestamp()} ${requestId} ${colors.blue}üöÄ Starting generation${colors.reset} | ${timeTrack(startTime)}`,
      );

      let messages = payload.messages;

      let params = {
        model: modelConfig.model || payload.model,
        messages,
        stream: payload.stream,
      } as {
        model: any;
        messages: Array<{ role: string; content: string }>;
        stream: boolean;
        system?: string;
        stopSequences?: string[];
        abortSignal?: AbortSignal;
      };

      // NOTE(jake): Unfortunately the o1 model is a unique snowflake, and requires
      // a distinctly different request payload than the other models..
      //
      // We can't send a system prompt, stop sequences, or max_tokens.
      if (payload.model?.startsWith("openai:o1")) {
        if (payload.system && messages.length > 0) {
          messages[0].content = `${payload.system}\n\n${messages[0].content}`;
        }
      } else {
        params = {
          ...params,
          system: payload.system,
          stopSequences: payload.stop ? [payload.stop] : undefined,
          abortSignal: request.signal,
        };
      }

      const llmStream = await streamText(params);

      let result = "";
      if (messages[messages.length - 1].role === "assistant") {
        result = messages[messages.length - 1].content;
      }

      let tokenCount = 0;

      if (payload.stream) {
        const stream = new ReadableStream({
          async start(controller) {
            // NOTE: the llm doesn't send text we put into its mouth, so we need to
            // manually send it so that streaming client sees everything assistant 'said'
            if (messages[messages.length - 1].role === "assistant") {
              controller.enqueue(new TextEncoder().encode(JSON.stringify(result) + '\n'));
            }
            for await (const delta of llmStream.textStream) {
              result += delta;
              tokenCount++;
              if (tokenCount % 100 === 0) {
                console.log(
                  `${timestamp()} ${requestId} ${colors.blue}üìä Generated${colors.reset} ${colors.bright}${tokenCount}${colors.reset} tokens | ${timeTrack(startTime)}`,
                );
              }
              controller.enqueue(
                new TextEncoder().encode(JSON.stringify(delta) + "\n"),
              );
            }

            console.log(
              `${timestamp()} ${requestId} ${colors.green}‚úÖ Stream complete:${colors.reset} ${colors.bright}${tokenCount}${colors.reset} tokens | ${timeTrack(startTime)}`,
            );

            if ((await llmStream.finishReason) === "stop" && payload.stop) {
              result += payload.stop;
              controller.enqueue(
                new TextEncoder().encode(JSON.stringify(payload.stop) + "\n"),
              );
            }

            if (messages[messages.length - 1].role === "user") {
              messages.push({ role: "assistant", content: result });
            } else {
              messages[messages.length - 1].content = result;
            }
            await cache.saveItem(cacheKey, params);
            controller.close();
          },
        });

        return new Response(stream, {
          headers: {
            "Content-Type": "text/event-stream",
            "Transfer-Encoding": "chunked",
          },
        });
      }

      for await (const delta of llmStream.textStream) {
        result += delta;
        tokenCount++;
      }

      console.log(
        `${timestamp()} ${requestId} ${colors.green}‚úÖ Generation complete:${colors.reset} ${colors.bright}${tokenCount}${colors.reset} tokens | ${timeTrack(startTime)}`,
      );

      if (!result) {
        console.error(
          `${timestamp()} ${requestId} ${colors.red}‚ùå No response from LLM${colors.reset} | ${timeTrack(startTime)}`,
        );
        return new Response(JSON.stringify({ error: "No response from LLM" }), {
          status: 500,
          headers: { "Content-Type": "application/json" },
        });
      }

      if ((await llmStream.finishReason) === "stop" && payload.stop) {
        result += payload.stop;
      }

      if (messages[messages.length - 1].role === "user") {
        messages.push({ role: "assistant", content: result });
      } else {
        messages[messages.length - 1].content = result;
      }

      await cache.saveItem(cacheKey, params);

      return new Response(
        JSON.stringify(params.messages[params.messages.length - 1]),
        {
          headers: { "Content-Type": "application/json" },
        },
      );
    } catch (error) {
      console.error(
        `${timestamp()} ${requestId} ${colors.red}‚ùå Error: ${(error as Error).message}${colors.reset} | ${timeTrack(startTime)}`,
      );
      return new Response(JSON.stringify({ error: (error as Error).message }), {
        status: 400,
        headers: { "Content-Type": "application/json" },
      });
    }
  } else {
    return new Response("Please send a POST request", { status: 405 });
  }
};

const port = Deno.env.get("PORT") || "8000";
console.log(`
${colors.bright}${colors.blue}üöÄ Planning Server Ready${colors.reset}
${colors.cyan}üåç http://localhost:${port}/${colors.reset}
${colors.yellow}üìù Cache directory: ${cache.CACHE_DIR}${colors.reset}
${colors.magenta}ü§ñ Available models: ${Object.keys(MODELS).join(", ")}${colors.reset}
`);
await serve(handler, { port: parseInt(port) });
